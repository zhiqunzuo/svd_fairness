{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import loader\n",
    "from models import BinningCalibrator\n",
    "import utils\n",
    "\n",
    "split_ratio_test = 0.3\n",
    "split_ratio_postproc = 0.1  # among all training data\n",
    "\n",
    "# These seeds control the randomness for the post-process/test split and in\n",
    "# postprocessing.  It does not affect pre-training data nor the randomness in\n",
    "# pre-training, i.e., we assume the pre-trained predictor to be fixed.  Results\n",
    "# will be aggregated over the seeds.\n",
    "seeds = range(33, 38)\n",
    "\n",
    "# This seed controls the randomness during pre-training (fixed)\n",
    "seed_pretrain = 33\n",
    "\n",
    "# Settings for calibration\n",
    "split_ratio_calib = 0.3\n",
    "n_bins_calib = 70\n",
    "\n",
    "max_workers = 4\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "\n",
    "data_dir = \"data/biasbios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Group</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accountant</th>\n",
       "      <td>2081</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>2398</td>\n",
       "      <td>7715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attorney</th>\n",
       "      <td>12494</td>\n",
       "      <td>20113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chiropractor</th>\n",
       "      <td>690</td>\n",
       "      <td>1908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comedian</th>\n",
       "      <td>592</td>\n",
       "      <td>2207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>composer</th>\n",
       "      <td>918</td>\n",
       "      <td>4682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dentist</th>\n",
       "      <td>5153</td>\n",
       "      <td>9326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dietitian</th>\n",
       "      <td>3689</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dj</th>\n",
       "      <td>211</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filmmaker</th>\n",
       "      <td>2310</td>\n",
       "      <td>4699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interior_designer</th>\n",
       "      <td>1183</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalist</th>\n",
       "      <td>9873</td>\n",
       "      <td>10077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>6214</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>17236</td>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>painter</th>\n",
       "      <td>3543</td>\n",
       "      <td>4193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paralegal</th>\n",
       "      <td>1499</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pastor</th>\n",
       "      <td>609</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_trainer</th>\n",
       "      <td>654</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photographer</th>\n",
       "      <td>8689</td>\n",
       "      <td>15635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physician</th>\n",
       "      <td>19579</td>\n",
       "      <td>18986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poet</th>\n",
       "      <td>3441</td>\n",
       "      <td>3570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professor</th>\n",
       "      <td>53290</td>\n",
       "      <td>64820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychologist</th>\n",
       "      <td>11385</td>\n",
       "      <td>6910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rapper</th>\n",
       "      <td>136</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_engineer</th>\n",
       "      <td>1089</td>\n",
       "      <td>5817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surgeon</th>\n",
       "      <td>1972</td>\n",
       "      <td>11301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teacher</th>\n",
       "      <td>9768</td>\n",
       "      <td>6428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga_teacher</th>\n",
       "      <td>1406</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Group              female   male\n",
       "Target                          \n",
       "accountant           2081   3571\n",
       "architect            2398   7715\n",
       "attorney            12494  20113\n",
       "chiropractor          690   1908\n",
       "comedian              592   2207\n",
       "composer              918   4682\n",
       "dentist              5153   9326\n",
       "dietitian            3689    289\n",
       "dj                    211   1274\n",
       "filmmaker            2310   4699\n",
       "interior_designer    1183    280\n",
       "journalist           9873  10077\n",
       "model                6214   1288\n",
       "nurse               17236   1735\n",
       "painter              3543   4193\n",
       "paralegal            1499    268\n",
       "pastor                609   1923\n",
       "personal_trainer      654    778\n",
       "photographer         8689  15635\n",
       "physician           19579  18986\n",
       "poet                 3441   3570\n",
       "professor           53290  64820\n",
       "psychologist        11385   6910\n",
       "rapper                136   1271\n",
       "software_engineer    1089   5817\n",
       "surgeon              1972  11301\n",
       "teacher              9768   6428\n",
       "yoga_teacher         1406    257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = [26 21  2 ... 18 15 13]\n",
      "example from attribute-aware dataset: {\n",
      "  \"bio\": \"Male. He produced scores of films including such as al-Dhareeh (the shrine), 1976, winner of the Cinema Institute Films\\u2019 Award at the Documentary and Short Films Festival, Egypt, 1977; as well as the Kelibia Festival Award, Tunisia, 1978; al-Mahatta (The Station), winner of a major award at Oberhausen Short Film Festival, Germany, 1989; the EU Award at FESPACO Festival, Burkina Faso, 1990; The Silver Sword Award at Damascus festival, 1990; and The Silver Tanit Award, Carthage festival, Tunisia, 1991. Eltayeb has served as head of the Sudanese Film group for several terms and as secretary of the Sudanese Film club. He has written numerous articles on cinema, published in major Sudanese newspapers. He is currently working on a long fiction film, al-Siraj wal-attama (The Lantern and Darkness).\",\n",
      "  \"title\": 9,\n",
      "  \"gender\": 1\n",
      "}\n",
      "example from attribute-blind dataset: {\n",
      "  \"bio\": \"He produced scores of films including such as al-Dhareeh (the shrine), 1976, winner of the Cinema Institute Films\\u2019 Award at the Documentary and Short Films Festival, Egypt, 1977; as well as the Kelibia Festival Award, Tunisia, 1978; al-Mahatta (The Station), winner of a major award at Oberhausen Short Film Festival, Germany, 1989; the EU Award at FESPACO Festival, Burkina Faso, 1990; The Silver Sword Award at Damascus festival, 1990; and The Silver Tanit Award, Carthage festival, Tunisia, 1991. Eltayeb has served as head of the Sudanese Film group for several terms and as secretary of the Sudanese Film club. He has written numerous articles on cinema, published in major Sudanese newspapers. He is currently working on a long fiction film, al-Siraj wal-attama (The Lantern and Darkness).\",\n",
      "  \"title\": 9,\n",
      "  \"gender\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Download BiasBios dataset\n",
    "\n",
    "(dataset, labels, label_names, groups,\n",
    " group_names) = loader.load_biasbios(data_dir, add_sensitive_attribute=True)\n",
    "dataset_u = loader.load_biasbios(data_dir, add_sensitive_attribute=False)[0]\n",
    "display(\n",
    "    loader.dataset_stats(dataset['title'], label_names, dataset['gender'],\n",
    "                         group_names))\n",
    "\n",
    "n_classes = len(label_names)\n",
    "n_groups = len(group_names)\n",
    "\n",
    "print('example from attribute-aware dataset:',\n",
    "      dumps(dataset[seed_pretrain], indent=2))\n",
    "print('example from attribute-blind dataset:',\n",
    "      dumps(dataset_u[seed_pretrain], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The table can't have duplicated columns but columns ['labels_ay'] are duplicated.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_pretrain \u001b[38;5;241m=\u001b[39m split_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Encode the joint (A, Y) labels by flattening\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m dataset_u \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_column\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels_ay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_u\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_u\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_u = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_u))\n\u001b[1;32m     16\u001b[0m split_dataset_u \u001b[38;5;241m=\u001b[39m dataset_u\u001b[38;5;241m.\u001b[39mtrain_test_split(\n\u001b[1;32m     17\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m split_ratio_test) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m split_ratio_postproc),\n\u001b[1;32m     18\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed_pretrain,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/svd/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd/lib/python3.10/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd/lib/python3.10/site-packages/datasets/arrow_dataset.py:5850\u001b[0m, in \u001b[0;36mDataset.add_column\u001b[0;34m(self, name, column, new_fingerprint)\u001b[0m\n\u001b[1;32m   5823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add column to Dataset.\u001b[39;00m\n\u001b[1;32m   5824\u001b[0m \n\u001b[1;32m   5825\u001b[0m \u001b[38;5;124;03m<Added version=\"1.7\"/>\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5847\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   5848\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5849\u001b[0m column_table \u001b[38;5;241m=\u001b[39m InMemoryTable\u001b[38;5;241m.\u001b[39mfrom_pydict({name: column})\n\u001b[0;32m-> 5850\u001b[0m \u001b[43m_check_column_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolumn_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5851\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_indices() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   5852\u001b[0m \u001b[38;5;66;03m# Concatenate tables horizontally\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd/lib/python3.10/site-packages/datasets/arrow_dataset.py:654\u001b[0m, in \u001b[0;36m_check_column_names\u001b[0;34m(column_names)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m count \u001b[38;5;129;01min\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    653\u001b[0m     duplicated_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m counter \u001b[38;5;28;01mif\u001b[39;00m counter[col] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe table can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have duplicated columns but columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicated_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are duplicated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The table can't have duplicated columns but columns ['labels_ay'] are duplicated."
     ]
    }
   ],
   "source": [
    "## Split data into (test + post-processing) and pre-training sets\n",
    "\n",
    "split_dataset = dataset.train_test_split(\n",
    "    test_size=(1 - split_ratio_test) * (1 - split_ratio_postproc),\n",
    "    seed=seed_pretrain,\n",
    ")\n",
    "dataset_ = split_dataset['train']\n",
    "dataset_pretrain = split_dataset['test']\n",
    "\n",
    "# Encode the joint (A, Y) labels by flattening\n",
    "dataset_u = dataset_u.add_column(\n",
    "    'labels_ay',\n",
    "    np.array(dataset_u['gender']) * n_classes + np.array(dataset_u['title']))\n",
    "\n",
    "split_dataset_u = dataset_u.train_test_split(\n",
    "    test_size=(1 - split_ratio_test) * (1 - split_ratio_postproc),\n",
    "    seed=seed_pretrain,\n",
    ")\n",
    "dataset_u_ = split_dataset_u['train']\n",
    "dataset_pretrain_u = split_dataset_u['test']\n",
    "\n",
    "labels_ = np.array(dataset_['title'])\n",
    "groups_ = np.array(dataset_['gender'])\n",
    "\n",
    "n_samples = len(dataset)\n",
    "n_test = int(n_samples * split_ratio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoying/anaconda3/envs/svd/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cecc9c4ca34bf0a22023cd6fdee45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cfee52f45c41adaae1f0e099d4ef54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee1226196314abfb7f4b6c05fc4887e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67affef05ef54b06a5b04cd23516b2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22b1114ed034a03a4fc73528cefcf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/145566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f4ef592ba445e2b21f454f4a172204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/247857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc306d3ad21a41c9b9796c7c53b031db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/145566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d876132f0b664c70a8ce9ca199186391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/247857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "  tokenized_examples = tokenizer(examples[\"bio\"],\n",
    "                                 padding=False,\n",
    "                                 max_length=tokenizer.model_max_length,\n",
    "                                 truncation=True)\n",
    "  return tokenized_examples\n",
    "\n",
    "\n",
    "tokenized_dataset_ = dataset_.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['bio'],\n",
    "    desc=\"Running tokenizer\",\n",
    ")\n",
    "tokenized_dataset_pretrain = dataset_pretrain.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['bio'],\n",
    "    desc=\"Running tokenizer\",\n",
    ")\n",
    "tokenized_dataset_u_ = dataset_u_.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['bio'],\n",
    "    desc=\"Running tokenizer\",\n",
    ")\n",
    "tokenized_dataset_pretrain_u = dataset_pretrain_u.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['bio'],\n",
    "    desc=\"Running tokenizer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "def train(model, dataset_train, dataset_test, label_col_name, batch_size,\n",
    "          n_epochs, lr, warmup_ratio, weight_decay, max_grad_norm):\n",
    "  dataloader_train = torch.utils.data.DataLoader(\n",
    "      dataset_train,\n",
    "      shuffle=True,\n",
    "      collate_fn=data_collator,\n",
    "      batch_size=batch_size,\n",
    "  )\n",
    "  dataloader_test = torch.utils.data.DataLoader(\n",
    "      dataset_test,\n",
    "      collate_fn=data_collator,\n",
    "      batch_size=batch_size,\n",
    "  )\n",
    "\n",
    "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "  optimizer_grouped_parameters = [\n",
    "      {\n",
    "          \"params\": [\n",
    "              p for n, p in model.named_parameters()\n",
    "              if not any(nd in n for nd in no_decay)\n",
    "          ],\n",
    "          \"weight_decay\": weight_decay,\n",
    "      },\n",
    "      {\n",
    "          \"params\": [\n",
    "              p for n, p in model.named_parameters()\n",
    "              if any(nd in n for nd in no_decay)\n",
    "          ],\n",
    "          \"weight_decay\": 0.0\n",
    "      },\n",
    "  ]\n",
    "  optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "  scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=(warmup_ratio * n_epochs * len(dataloader_train)),\n",
    "      num_training_steps=n_epochs * len(dataloader_train))\n",
    "\n",
    "  model_input_args = list(model.forward.__code__.co_varnames)\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader_train, desc=f\"train epoch {epoch+1}\"):\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(**{\n",
    "          k: v for k, v in batch.items() if k in model_input_args\n",
    "      })\n",
    "      loss = loss_fn(outputs.logits, batch[label_col_name])\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      test_loss = 0\n",
    "      test_acc = 0\n",
    "      for batch in tqdm(dataloader_test, desc=f\"test {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**{\n",
    "            k: v for k, v in batch.items() if k in model_input_args\n",
    "        })\n",
    "        probas = outputs.logits\n",
    "        test_loss += loss_fn(probas, batch[label_col_name]).item()\n",
    "        probas_y = probas.softmax(dim=1).reshape(len(batch['title']), -1,\n",
    "                                                 n_classes).sum(dim=1)\n",
    "        test_acc += (probas_y.argmax(dim=1) == batch['title']).sum().item()\n",
    "      test_loss /= len(dataset_test['title'])\n",
    "      test_acc /= len(dataset_test['title'])\n",
    "      print(\n",
    "          f\"epoch {epoch+1}/{n_epochs}: loss={test_loss:.4f}, acc={test_acc:.4f}\"\n",
    "      )\n",
    "\n",
    "\n",
    "def predict_probas(model, dataset, batch_size):\n",
    "  dataloader = torch.utils.data.DataLoader(\n",
    "      dataset,\n",
    "      collate_fn=data_collator,\n",
    "      batch_size=batch_size,\n",
    "  )\n",
    "  model_input_args = list(model.forward.__code__.co_varnames)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    probas = []\n",
    "    for batch in tqdm(dataloader, desc=\"inference\"):\n",
    "      batch = {\n",
    "          k: v.to(device) for k, v in batch.items() if k in model_input_args\n",
    "      }\n",
    "      outputs = model(**batch)\n",
    "      probas.append(outputs.logits.softmax(dim=1).cpu().numpy())\n",
    "    probas = np.concatenate(probas, axis=0)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_evaluation(model, dataset, batch_size):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    model_input_args = list(model.forward.__code__.co_varnames)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probas = []\n",
    "        genders = []\n",
    "        titles = []\n",
    "        iters = 0\n",
    "        for batch in tqdm(dataloader, desc=\"inference\"):\n",
    "            genders.append(batch[\"gender\"])\n",
    "            titles.append(batch[\"title\"])\n",
    "            batch = {\n",
    "                k: v.to(device) for k, v in batch.items() if k in model_input_args\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            probas.append(outputs.logits.softmax(dim=1).cpu().numpy())\n",
    "            iters += 1\n",
    "            if iters > 2:\n",
    "                break\n",
    "        probas = np.concatenate(probas, axis=0)\n",
    "        genders = np.concatenate(genders, axis=0)\n",
    "        titles = np.concatenate(titles, axis=0)\n",
    "        \n",
    "    predicts = np.argmax(probas, axis=1)\n",
    "    total_acc = np.sum(predicts == titles) / titles.shape[0]\n",
    "    print(\"acc = {}\".format(total_acc))\n",
    "    \n",
    "    predicts_1 = predicts[genders == 0]\n",
    "    titles_1 = titles[genders == 0]\n",
    "    acc_1 = np.sum(predicts_1 == titles_1) / titles_1.shape[0]\n",
    "    predicts_2 = predicts[genders == 1]\n",
    "    titles_2 = titles[genders == 1]\n",
    "    acc_2 = np.sum(predicts_2 == titles_2) / titles_2.shape[0]\n",
    "    print(\"DP disparity = {}\".format(abs(acc_1 - acc_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoying/anaconda3/envs/svd/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m transformers\u001b[38;5;241m.\u001b[39mset_seed(seed_pretrain)\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     model_name, num_labels\u001b[38;5;241m=\u001b[39mn_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     16\u001b[0m fairness_evaluation(model, tokenized_dataset_\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed_pretrain)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mtrain(\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_dict'"
     ]
    }
   ],
   "source": [
    "## (Pre-)train predictors\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 3\n",
    "lr = 2e-5\n",
    "warmup_ratio = 0.1\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Train attribute-aware p(Y | X) predictor\n",
    "transformers.set_seed(seed_pretrain)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=n_classes).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"full_model.pt\"))\n",
    "fairness_evaluation(model, tokenized_dataset_.train_test_split(test_size=0.1, seed=seed_pretrain)[\"test\"], batch_size)\n",
    "\n",
    "\"\"\"\n",
    "train(\n",
    "    model,\n",
    "    tokenized_dataset_pretrain,\n",
    "    tokenized_dataset_.train_test_split(test_size=0.1,\n",
    "                                        seed=seed_pretrain)['test'],\n",
    "    'title',\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    warmup_ratio,\n",
    "    weight_decay,\n",
    "    max_grad_norm,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Train attribute-blind p(A, Y | X) predictor\n",
    "transformers.set_seed(seed_pretrain)\n",
    "model_u = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=n_groups * n_classes).to(device)\n",
    "\n",
    "train(\n",
    "    model_u,\n",
    "    tokenized_dataset_pretrain_u,\n",
    "    tokenized_dataset_u_.train_test_split(test_size=0.1,\n",
    "                                          seed=seed_pretrain)['test'],\n",
    "    'labels_ay',\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    warmup_ratio,\n",
    "    weight_decay,\n",
    "    max_grad_norm,\n",
    ")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
